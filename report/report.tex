\documentclass{article}

\usepackage{tikz} 
\usetikzlibrary{automata, positioning, arrows} 

\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{color}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{float}
\usepackage{forest}

\usepackage[section]{placeins}
  \hypersetup{
    colorlinks = true,
    urlcolor = blue,
    linkcolor= blue,
    citecolor= blue,
    filecolor= blue,
    }
    
\usepackage{listings}
\usepackage[utf8]{inputenc}                                                    
\usepackage[T1]{fontenc}                                                       

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=haskell,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newtheoremstyle{theorem}
  {\topsep}   % ABOVESPACE
  {\topsep}   % BELOWSPACE
  {\itshape\/}  % BODYFONT
  {0pt}       % INDENT
  {\bfseries} % HEADFONT
  {.}         % HEADPUNCT
  {5pt plus 1pt minus 1pt} % HEADSPACE
  {}
\theoremstyle{plain} 
   \newtheorem{theorem}{Theorem}[section]
   \newtheorem{corollary}[theorem]{Corollary}
   \newtheorem{lemma}[theorem]{Lemma}
   \newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
   \newtheorem{definition}[theorem]{Definition}
   \newtheorem{example}[theorem]{Example}
\theoremstyle{remark}    
  \newtheorem{remark}[theorem]{Remark}

\title{CPSC-354 Report}
\author{Ethan Tapia  \\ Chapman University}

\date{\today} 

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\setcounter{tocdepth}{3}
\tableofcontents

\section{Introduction}\label{intro}

\section{Week by Week}\label{homework}

% =========================
% Week 1 
% =========================
\subsection{Week 1}

\textbf{Lecture Summary}

We introduced \emph{formal systems} and worked with Hofstadter’s MIU-system as a rule–based rewriting game.  
Alphabet: $\Sigma=\{M,I,U\}$.  
Axiom (start string): $MI$.  
Production rules:
\begin{enumerate}
    \item[\textbf{(R1)}] If a string ends in $I$, append $U$: $xI \Rightarrow xIU$.
    \item[\textbf{(R2)}] If a string is $Mx$, duplicate $x$: $Mx \Rightarrow Mxx$.
    \item[\textbf{(R3)}] Replace any $III$ by $U$: $xIIIy \Rightarrow xUy$.
    \item[\textbf{(R4)}] Delete any $UU$: $xUUy \Rightarrow xy$.
\end{enumerate}
Key idea: reason about \emph{invariants} that rules preserve, instead of searching blindly through derivations.

\bigskip
\textbf{Homework: The MU-puzzle}

\begin{definition}[I–count and residue]
For a string $w$, let $\#_I(w)$ be the number of $I$’s in $w$, and define the residue
\[
\varphi(w) \;=\; \#_I(w) \bmod 3 \in \{0,1,2\}.
\]
\end{definition}

\begin{lemma}[Effect of each rule on $\#_I$]\label{lem:rule-effects}
For any string $w$:
\begin{enumerate}
    \item \textbf{(R1)} and \textbf{(R4)} do not change $\#_I$.
    \item \textbf{(R2)} doubles the number of $I$’s \emph{after} the initial $M$, so $\varphi$ is multiplied by $2$ modulo $3$.
    \item \textbf{(R3)} decreases $\#_I$ by $3$, so $\varphi$ is unchanged.
\end{enumerate}
\end{lemma}

\begin{proposition}[Invariant modulo $3$]\label{prop:invariant}
Every string derivable from $MI$ has $\varphi\in\{1,2\}$. In particular, no derivable string has $\varphi=0$.
\end{proposition}

\begin{proof}
We use induction on the length of a derivation from $MI$.

\emph{Base.} $\varphi(MI)=1$.

\emph{Step.} Assume $\varphi\in\{1,2\}$ for some derivable $w$.  
By Lemma~\ref{lem:rule-effects}, rules (R1), (R3), and (R4) keep $\varphi$ unchanged, and rule (R2) maps $1\leftrightarrow 2$ modulo $3$. None of these operations yields $0$ from a value in $\{1,2\}$. Therefore the next string also has $\varphi\in\{1,2\}$.
\end{proof}

\begin{theorem}[MU is unreachable]
\label{thm:mu-unreachable}
$MU$ cannot be derived from $MI$ in the MIU-system.
\end{theorem}

\begin{proof}
$MU$ contains zero $I$’s, hence $\varphi(MU)=0$. By Proposition~\ref{prop:invariant}, every derivable string has residue $1$ or $2$. Thus $MU$ is not derivable.
\end{proof}

\textit{Conclusion.} Starting from $MI$ we can toggle the residue $1\leftrightarrow 2$ with (R2) and otherwise keep it fixed with (R1), (R3), (R4). We never reach residue $0$, so no sequence of legal rule applications yields $MU$.

\begin{center}
\begin{tikzpicture}[node distance=3cm, auto]
  \tikzstyle{state}=[circle,draw,minimum size=9mm]
  \node[state, label=below:{residue of $\#I$}] (Z1) {1};
  \node[state, right=of Z1] (Z2) {2};
  \node[state, right=of Z2] (Z0) {0};
  \path[->, thick]
    (Z1) edge[bend left] node[above]{(R2)} (Z2)
    (Z2) edge[bend left] node[below]{(R2)} (Z1);
  \path[->, dashed]
    (Z1) edge[loop above] node{R1,R3,R4} ()
    (Z2) edge[loop above] node{R1,R3,R4} ()
    (Z0) edge[loop above] node{R1,R3,R4} ();

\end{tikzpicture}
\end{center}
\textbf{Question:} If the MU-puzzle shows that some goals are unreachable due to invariants (like the mod-3 property of I’s), how does this idea connect to undecidability in programming languages?

\FloatBarrier

% =========================
% Week 2 
% =========================
\subsection{Week 2}

\textbf{Lecture Summary} \\
We introduced \emph{Abstract Reduction Systems (ARS)}: a pair $(A,R)$ with one-step reduction $R\subseteq A\times A$. Key notions:
reducible/normal form, joinability, confluence, termination, and unique normal forms.

\bigskip
\textbf{Homework Part 2: The 8 Combinations}

We provide an example ARS for each combination of
$(\text{confluent}, \text{terminating}, \text{unique NFs})$.
If a row is impossible, we explain why.

\begin{center}
\begin{tabular}{|c|c|c|l|}
\hline
\textbf{Confluent} & \textbf{Terminating} & \textbf{Unique NFs} & \textbf{Example} \\
\hline
True  & True  & True  & $A=\{a\},\ R=\emptyset$ (Fig.~\ref{fig:combo-ttt}) \\
True  & True  & False & \emph{Impossible} \\
True  & False & True  & $A=\{a,b\},\ R=\{(a,a),(a,b)\}$ (Fig.~\ref{fig:combo-tft}) \\
True  & False & False & $A=\{a\},\ R=\{(a,a)\}$ (Fig.~\ref{fig:combo-tff}) \\
False & True  & True  & \emph{Impossible} \\
False & True  & False & $A=\{a,b,c\},\ R=\{(a,b),(a,c)\}$ (Fig.~\ref{fig:combo-ftf}) \\
False & False & True  & \emph{Impossible} \\
False & False & False & $A=\{a,b,c\},\ R=\{(a,b),(a,c),(b,b),(c,c)\}$ (Fig.~\ref{fig:combo-fff}) \\
\hline
\end{tabular}
\end{center}

\noindent\textit{Why some rows are impossible.}
If an ARS has unique normal forms, it must be confluent.
If an ARS is both confluent and terminating, then every element reduces to a unique normal form.
Therefore the rows \((\text{T},\text{T},\text{F})\), \((\text{F},\text{T},\text{T})\), and \((\text{F},\text{F},\text{T})\) cannot occur.

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
\end{tikzpicture}
\caption{Combination (True, True, True). Terminating, confluent, unique NF.}
\label{fig:combo-ttt}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
  \node[obj] (b) [right=of a] {$b$};
  \path (a) edge (b)
        (a) edge[loop above] (a);
\end{tikzpicture}
\caption{Combination (True, False, True). Non-terminating, confluent, unique NF $b$.}
\label{fig:combo-tft}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
  \path (a) edge[loop right] (a);
\end{tikzpicture}
\caption{Combination (True, False, False). Non-terminating, confluent, no normal form.}
\label{fig:combo-tff}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3.2cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
  \node[obj] (b) [right=of a,yshift=10pt] {$b$};
  \node[obj] (c) [right=of a,yshift=-10pt] {$c$};
  \path (a) edge (b)
        (a) edge (c);
\end{tikzpicture}
\caption{Combination (False, True, False). Terminating, not confluent; two distinct normal forms $b,c$ are not joinable.}
\label{fig:combo-ftf}
`\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3.2cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
  \node[obj] (b) [right=of a,yshift=10pt] {$b$};
  \node[obj] (c) [right=of a,yshift=-10pt] {$c$};
  \path (a) edge (b)
        (a) edge (c)
        (b) edge[loop right] (b)
        (c) edge[loop right] (c);
\end{tikzpicture}
\caption{Combination (False, False, False). Non-terminating (loops), not confluent, no unique normal forms.}
\label{fig:combo-fff}
\end{figure}

\FloatBarrier

\noindent\paragraph{Conclusion.}
The MU-puzzle illustrates how invariants prove impossibility in a formal system.
The ARS framework provides the general language to study rewrite systems via termination, confluence, and normal forms.
The 8-combination analysis shows which behaviors are possible and which are structurally impossible.

\noindent\paragraph{Question:}
Could there be a general framework that unifies invariants with confluence and termination, so that impossibility and determinism appear as two sides of the same rewriting theory?
\subsection{Week 3}
\textbf{Lecture Summary}
\\TBD

\textbf{Homework 3}
\paragraph{Exercise 5}

Consider an ARS with
\[
A = \{a,b\}^* = \{\varepsilon, a, b, aa, ab, ba, bb, aaa, \dots \}
\]
and rewrite rules
\[
ab \to ba, \qquad 
ba \to ab, \qquad 
aa \to \varepsilon, \qquad 
b \to \varepsilon
\]

\begin{enumerate}
  \item \textbf{Reduce some example strings such as $abba$ and $bababa$.}
  \[
    abba \to aa \to \varepsilon, \qquad
    bababa \to aaa \to a
  \]

  \item \textbf{Find two strings that are not equivalent. How many non-equivalent strings can you find?}
  \begin{itemize}
    \item $\varepsilon$
    \item $a$
  \end{itemize}
  These have different normal forms and cannot be transformed into each other.

  \item \textbf{How many equivalence classes does $\stackrel{\ast}{\longleftrightarrow}$ have? What are the normal forms?} \\
  There are two equivalence classes:
  \begin{enumerate}
    \item Strings whose normal form is $\varepsilon$,
    \item Strings whose normal form is $a$.
  \end{enumerate}
  The class is determined by the parity of the number of $a$’s in the string.

  \item \textbf{Can you modify the ARS so that it becomes terminating without changing its equivalence classes?} \\
  Yes. Remove one of the first two rules. They only permute $a$ and $b$ and do not affect equivalence classes, but having both makes the system non-terminating.

  \item \textbf{Question:} \\
  If I remove all the $b$’s from a string, does the remaining word reduce to $a$ or to $\varepsilon$?  

  \textbf{Answer:} This can be answered using the ARS because $b \to \varepsilon$ always deletes $b$’s, and the final result depends only on whether the number of $a$’s left is odd or even. Odd $\mapsto a$, even $\mapsto \varepsilon$.
\end{enumerate}

\paragraph{Exercise 5b}

Now replace the rule $aa \to \varepsilon$ with $aa \to a$.

\begin{enumerate}
  \item \textbf{Reduce some example strings such as $abba$ and $bababa$.}
  \[
    abba \to aa \to a, \qquad
    bababa \to aaa \to aa \to a
  \]

  \item \textbf{Find two strings that are not equivalent.}
  \begin{itemize}
    \item $\varepsilon$
    \item $a$
  \end{itemize}

  \item \textbf{How many equivalence classes are there? What are the normal forms?} \\
  There are two equivalence classes:
  \begin{enumerate}
    \item Strings with no $a$’s $\mapsto$ normal form $\varepsilon$,
    \item Strings with at least one $a$ $\mapsto$ normal form $a$.
  \end{enumerate}

  \item \textbf{Modify the ARS to make it terminating.} \\
  As above, remove one of the two swapping rules $ab \leftrightarrow ba$.

  \item \textbf{Question:} \\
  Is the system confluent? That is, if a string can be reduced in two different ways, do the reductions always lead to the same normal form?
\end{enumerate}

\subsection{Week 4}
\textbf{Lecture Summary}
\\An \emph{invariant} is a function or property that remains unchanged under the rewriting relation of an ARS. 
They are central tools across science (e.g.\ conservation laws in physics, chemistry, and biology) and mathematics. 
Formally, $P:A\to B$ is an invariant if $a\to b \Rightarrow P(a)=P(b)$. 
Strong invariants preserve exact equality, while weak invariants preserve truth of properties. 
Invariants induce partitions on $A$, often serving as abstractions of the equivalence relation $\leftrightarrow^\ast$. 
They can be used to prove impossibility (show $P(a)=\text{true}$, $P(b)=\text{false}$) and to build \emph{complete invariants}, which fully classify equivalence classes. 
Examples include letter counts in string rewriting systems and parity arguments in puzzles (domino tilings, sliding puzzles). 
In programming, invariants explain correctness of while-loops and recursion, while measure functions guarantee termination.


\textbf{Homework 4.1}\\
\textbf{Algorithm}
\begin{verbatim}
while b != 0:
  temp = b
  b = a mod b
  a = temp
return a
\end{verbatim}

\paragraph{Conditions under which it always terminates.}
Assume \(a,b\in\mathbb{N}\) with \(b\ge 0\). If \(b=0\) the loop does not run and the program returns immediately. If \(b>0\) then each loop iteration is well defined and yields a strictly smaller nonnegative \(b\) because \(a \bmod b\in\{0,1,\dots,b-1\}\). Thus the loop must terminate. (Equivalently: Euclid’s algorithm terminates for all nonnegative integers, not both zero.)

\paragraph{Measure function and proof.}
Let the state be the pair \((a,b)\in\mathbb{N}^2\). Define
\[
\phi(a,b)=b.
\]
Suppose the guard holds, so \(b>0\). One loop step computes
\[
(a',b')=(b,\; a\bmod b).
\]
Then \(0\le b' < b\), hence \(\phi(a',b')=b' < b=\phi(a,b)\).
Therefore \(\phi\) strictly decreases on every iteration while staying in \(\mathbb{N}\). Since \(>\) on \(\mathbb{N}\) is well founded, no infinite descent exists, so the loop terminates.

\textbf{Homework 4.2}\\
\textbf{Fragment}
\begin{verbatim}
function merge_sort(arr, left, right):
  if left >= right:
    return
  mid = (left + right) / 2   // integer division
  merge_sort(arr, left, mid)
  merge_sort(arr, mid+1, right)
  merge(arr, left, mid, right)
\end{verbatim}

\paragraph{Claim.}
\(\displaystyle \phi(left,right)=right-left+1\) is a measure function for the recursive calls of \texttt{merge\_sort}.

\paragraph{Proof.}
We reason about the domain \(D=\{(l,r)\in\mathbb{Z}^2 \mid l\le r\}\) with the measure \(\phi(l,r)=r-l+1\in\mathbb{N}\).

If \(left\ge right\) then \(\phi(left,right)\in\{0,1\}\) and the function returns, so there is no recursive descent.

Assume \(left<right\). Let \(mid=\lfloor (left+right)/2\rfloor\). Standard bounds give
\[
left \le mid < right \quad\text{and}\quad left < mid+1 \le right.
\]
Hence both subranges are valid:
\[
(left,mid)\in D, \qquad (mid+1,right)\in D.
\]
Their measures satisfy
\[
\phi(left,mid)=mid-left+1 \le \left\lfloor\frac{left+right}{2}\right\rfloor-left+1
< \frac{left+right}{2}-left+1
= \frac{right-left+2}{2} \le right-left,
\]
so \(\phi(left,mid) \le right-left < right-left+1=\phi(left,right)\). Similarly,
\[
\phi(mid+1,right)=right-(mid+1)+1=right-mid
\le right-\left\lfloor\frac{left+right}{2}\right\rfloor
< right-\frac{left+right}{2}
= \frac{right-left}{2}
< right-left+1=\phi(left,right).
\]
Thus each recursive argument strictly decreases the measure \(\phi\). Since \(\phi\) takes values in \(\mathbb{N}\) and strictly decreases along every recursion chain, the recursion is well founded and \texttt{merge\_sort} terminates.

\textbf{Question:} \\
We can discovered that Euclid’s algorithm always stops. But how could you use an invariant to also show that it actually gives the greatest common divisor, not just any number?

\subsection{Week 5}
\textbf{Lecture Summary}\\
Lambda calculus is a minimal but Turing-complete language with only three constructs: abstraction ($\lambda x.e$ defines a function), application ($e_1\,e_2$ applies a function to an argument), and variables (simple names without assignment). Application associates to the left and abstraction chains naturally. Computation is substitution: $(\lambda x.M)\,N \rightsquigarrow M[N/x]$ (the $\beta$-rule), with bound variables freely renamable ($\alpha$-equivalence) to avoid capture. Functions can return functions (currying), and using Church encodings, numbers and arithmetic can be represented purely by substitution.


\textbf{Homework 5: Lambda Calculus Reduction}\\
\textbf{We Evaluate:}
\[
(\lambda f.\,\lambda x.\, f(f(x))) \; (\lambda f.\,\lambda x.\, f(f(f(x))))
\]
\noindent
\textbf{Step 1: Rename the boud variables of the second term to avoid clashes} 
\[
(\lambda f.\,\lambda x.\, f(f(x))) \; (\lambda g.\,\lambda y.\, g(g(g(y))))
\]

\noindent
\textbf{Step 2: Apply the outer function to its argument}
\[
\lambda x.\, (\lambda g.\,\lambda y.\, g(g(g(y)))) \big( (\lambda g.\,\lambda y.\, g(g(g(y))))\, x \big)
\]

\noindent
\textbf{Step 3: Reduce the inner application}
\[
\lambda x.\, (\lambda g.\,\lambda y.\, g(g(g(y)))) \, (\lambda y.\, x(x(xy)))
\]

\noindent
\textbf{Step 4: Apply again}
\[
\lambda x.\,\lambda y.\, (\lambda y.\, x(x(xy))) \big((\lambda y.\, x(x(xy))) \, ((\lambda y.\, x(x(xy)))\, y)\big)
\]

\noindent
\textbf{Step 5: Evaluate the nested calls}
\[
\lambda x.\,\lambda y.\, x(x(x(x(x(x(x(x(xy)))))))))
\]

\noindent
\textbf{Final result.} This is the Church numeral
\[
\lambda f.\,\lambda x.\, f^9(x)
\]
This is the number $9$ in Church encoding.

\medskip
\noindent
\emph{Note:} The workout shows that $2\,3 = 9$ for Church numerals. In general, Church numerals encode repeated function application, and application corresponds to multiplication.

\textbf{Question:} If variable names don’t matter in $\lambda$
-calculus, what does that suggest about how meaning can exist independently of representation?

\subsection{Week 6}

\textbf{Lecture Summary}\\  
This lecture introduced recursion in the $\lambda$-calculus via the \textit{fixed point combinator}. We learned that recursion can be encoded without special syntax by defining \texttt{fix} such that $\texttt{fix}\ F \to F(\texttt{fix}\ F)$. Using this, one can define recursive functions like factorial. We also reviewed the definitions of \texttt{let} and \texttt{let rec}, which expand into $\lambda$-abstractions and applications of \texttt{fix}. The key point is that recursion in functional languages comes from self-application and fixed points, with the famous $Y$-combinator as a canonical construction.

\textbf{Homework 6: Fixed Points and Recursion}\\

\textbf{Rules:}\\
\[
\begin{aligned}
\texttt{fix}\ F &\;\to\; F\,(\texttt{fix}\ F) \qquad &\textbf{(def of fix)}\\
\texttt{let}\ x=e_1\ \texttt{in}\ e_2 &\;\to\; (\lambda x.\,e_2)\ e_1 \qquad &\textbf{(def of let)}\\
\texttt{let rec}\ f=e_1\ \texttt{in}\ e_2 &\;\to\; \texttt{let}\ f=(\texttt{fix}\ (\lambda f.\,e_1))\ \texttt{in}\ e_2 \qquad &\textbf{(def of let rec)}
\end{aligned}
\]

\textbf{Abbreviation.}  
For readability set
\[
G \;\equiv\; \lambda f.\,\lambda n.\, \texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n * f(n-1).
\]

\textbf{Goal term.}
\[
\texttt{let rec}\ \texttt{fact} = \lambda n.\, \texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n * \texttt{fact}(n-1)\ \texttt{in}\ \texttt{fact}\ 3
\]

\textbf{Derivation}
\[
\begin{aligned}
&\texttt{let rec}\ \texttt{fact} = \lambda n.\,\texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n * \texttt{fact}(n-1)\ \texttt{in}\ \texttt{fact}\ 3
\\
\to\;& \texttt{let}\ \texttt{fact} = \texttt{fix}\,G\ \texttt{in}\ \texttt{fact}\ 3 \quad \textbf{\small<def of let rec>}
\\
\to\;& (\lambda \texttt{fact}.\, \texttt{fact}\ 3)\ (\texttt{fix}\,G) \quad \textbf{\small<def of let>}
\\
\to\;& (\texttt{fix}\,G)\ 3 \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& (G(\texttt{fix}\,G))\ 3 \quad \textbf{\small<def of fix>}
\\
\to\;& (\lambda n.\, \texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n*(\texttt{fix}\,G)(n-1))\ 3 \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& \texttt{if}\ 3=0\ \texttt{then}\ 1\ \texttt{else}\ 3 * (\texttt{fix}\,G)(2) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (\texttt{fix}\,G)(2) \quad \textbf{\small<def of if>}
\\
\to\;& 3 * \big(G(\texttt{fix}\,G)\big)\ 2 \quad \textbf{\small<def of fix>}
\\
\to\;& 3 * (\lambda n.\,\texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n*(\texttt{fix}\,G)(n-1))\ 2 \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (\texttt{if}\ 2=0\ \texttt{then}\ 1\ \texttt{else}\ 2*(\texttt{fix}\,G)(1)) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (\texttt{fix}\,G)(1)) \quad \textbf{\small<def of if>}
\\
\to\;& 3 * (2 * (G(\texttt{fix}\,G))\ 1) \quad \textbf{\small<def of fix>}
\\
\to\;& 3 * (2 * (\lambda n.\,\texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n*(\texttt{fix}\,G)(n-1))\ 1) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (\texttt{if}\ 1=0\ \texttt{then}\ 1\ \texttt{else}\ 1*(\texttt{fix}\,G)(0))) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (1 * (\texttt{fix}\,G)(0))) \quad \textbf{\small<def of if>}
\\
\to\;& 3 * (2 * (1 * (G(\texttt{fix}\,G))\ 0)) \quad \textbf{\small<def of fix>}
\\
\to\;& 3 * (2 * (1 * (\lambda n.\,\texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n*(\texttt{fix}\,G)(n-1))\ 0)) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (1 * (\texttt{if}\ 0=0\ \texttt{then}\ 1\ \texttt{else}\ 0*(\texttt{fix}\,G)(-1)))) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (1 * 1)) \quad \textbf{\small<def of if>}
\\
\to\;& 3 * (2 * 1) \quad \textbf{\small<arith>}
\\
\to\;& 3 * 2 \quad \textbf{\small<arith>}
\\
\to\;& 6 \quad \textbf{\small<arith>}
\end{aligned}
\]

\textbf{Result:} \texttt{fact 3} reduces to $6$, each step justified by \textbf{def of let rec}, \textbf{def of let}, \(\beta\)\textbf{-rule}, \textbf{def of fix}, \textbf{def of if}, and \textbf{arith}.

\textbf{Question:} Since the fixed point combinator allows functions to call themselves without being named, what does this suggest about the nature of recursion and whether naming is essential for defining self-reference?

\subsection{Week 7}\\
\textbf{Lecture Summary}\\
This lecture introduced parsing and context-free grammars (CFGs) using the calculator example. Parsing was explained as the process of turning concrete syntax (strings) into abstract syntax (trees). We saw how CFG rules capture precedence and associativity, and how parse trees differ from simplified abstract syntax trees (ASTs). Lisp was discussed as a language where programmers essentially write abstract syntax directly. \\

\textbf{Homework: Parse Trees for Arithmetic Expressions}

\textbf{1. Expression: $2+1$}

\begin{forest}
[Exp
  [Exp
    [Exp1
      [Exp2
        [Integer [2]]]]]
  [+]
  [Exp1
    [Exp2
      [Integer [1]]]]]
\end{forest}

\textbf{2. Expression: $1+2*3$}

\begin{forest}
[Exp
  [Exp
    [Exp1
      [Exp2
        [Integer [1]]]]]
  [+]
  [Exp1
    [Exp1
      [Exp2 [Integer [2]]]]
    [*]
    [Exp2 [Integer [3]]]]]
\end{forest}

\textbf{3. Expression: $1+(2*3)$}

\begin{forest}
[Exp
  [Exp
    [Exp1
      [Exp2
        [Integer [1]]]]]
  [+]
  [Exp1
    [Exp2
      [(]
        [Exp
          [Exp1
            [Exp1
              [Exp2 [Integer [2]]]]
            [*]
            [Exp2 [Integer [3]]]]]
      [)]]]]
\end{forest}

\textbf{4. Expression: $(1+2)*3$}

\begin{forest}
[Exp
  [Exp1
    [Exp1
      [Exp2
        [(]
          [Exp
            [Exp
              [Exp1
                [Exp2 [Integer [1]]]]]
            [+]
            [Exp1
              [Exp2 [Integer [2]]]]]
        [)]]]
    [*]
    [Exp2 [Integer [3]]]]]
\end{forest}


\textbf{5. Expression: $1+2*3+4*5+6$}

\begin{forest}
[Exp
  [Exp
    [Exp
      [Exp1 [Exp2 [Integer [1]]]]
      [+]
      [Exp1
        [Exp1 [Exp2 [Integer [2]]]]
        [*]
        [Exp2 [Integer [3]]]]]
    [+]
    [Exp1
      [Exp1 [Exp2 [Integer [4]]]]
      [*]
      [Exp2 [Integer [5]]]]]
  [+]
  [Exp1
    [Exp2 [Integer [6]]]]]
\end{forest}

\textbf{Question:} If Lisp lets programmers write abstract syntax directly, what does this reveal about the trade-offs between readability for humans and ease of parsing for machines?

\subsection{Week 8}
\textbf{Lecture Summary}

This lecture introduced formal proofs in Lean using the natural numbers tutorial world. We practiced rewriting with lemmas such as \texttt{add\_zero} and \texttt{add\_succ}, and learned how to control which occurrence gets rewritten. The exercises illustrated how even simple arithmetic like $2 + 2 = 4$ requires careful step-by-step rewriting when starting from the axioms of Peano arithmetic. The key insight is that Lean forces us to be explicit about each rule application, which deepens our understanding of how proofs are built from small definitional steps.

\textbf{Homework 8: Lean Tutorial World (Levels 5--8)}

\textit{Level 5.} Prove $a + (b + 0) + (c + 0) = a + b + c$.

\begin{verbatim}
rw [add_zero c]
rw [add_zero]
rfl
\end{verbatim}

\textit{Level 6.} Prove $succ n = n + 1$.

\begin{verbatim}
rw [one_eq_succ_zero]
rw [add_succ]
rw [add_zero]
rfl
\end{verbatim}

\textit{Level 7.} Prove $2 + 2 = 4$.

\begin{verbatim}
rw [two_eq_succ_one]
rw [add_succ]
rw [one_eq_succ_zero]
rw [add_succ]
rw [add_zero]
rw [four_eq_succ_three]
rw [three_eq_succ_two]
rw [two_eq_succ_one]
rw [one_eq_succ_zero]
rfl
\end{verbatim}

\textit{Level 8 (alternative short proof).} Another valid solution using \texttt{nth\_rewrite} and reversed rewrites.

\begin{verbatim}
nth_rewrite 2 [two_eq_succ_one]
rw [add_succ]
rw [one_eq_succ_zero]
rw [add_succ, add_zero]
rw [← three_eq_succ_two]
rw [← four_eq_succ_three]
rfl
\end{verbatim}

\textbf{Question:} When Lean forces us to spell out each small step (like showing $2 + 2 = 4$ from the Peano axioms), it reveals how much structure is hidden in even basic arithmetic. What does this suggest about the trade-off between human mathematical intuition (where $2 + 2 = 4$ is obvious) and machine-checked rigor (where nothing is obvious until proved)?

\subsection{Week 9}
\textbf{Lecture Summary}

This lecture focused on proving properties of addition using associativity and commutativity. We learned how Lean allows us to move brackets with \texttt{add\_assoc} and reorder terms with \texttt{add\_comm}, avoiding induction in many cases. At the same time, we also practiced writing an inductive proof, showing how different proof strategies can establish the same result. The key takeaway is that structural lemmas like associativity and commutativity make proofs shorter and more direct, but induction remains a powerful general method.

\textbf{Homework 9: Lean Tutorial World (Level 5)}

\textit{Goal:} Prove $(a + b) + c = a + (c + b)$.

---

\textit{Solution 1 (without induction).}

\begin{verbatim}
rw [add_assoc]
rw [add_comm b c]
rw [add_assoc]
rfl
\end{verbatim}

\textbf{Mathematical proof:}  
Starting with $(a + b) + c$, by associativity we have $a + (b + c)$.  
By commutativity of addition, this equals $a + (c + b)$.  
By associativity again, this is $(a + c) + b$, which proves the equality.

---

\textit{Solution 2 (with induction).}

\begin{verbatim}
induction c with d hd,
{ rw [add_zero], rw [add_zero], rfl },
{ rw [add_succ], rw [add_succ], rw hd, rfl }
\end{verbatim}

\textbf{Mathematical proof:}  
We prove by induction on $c$ that $(a + b) + c = a + (c + b)$ for all $a, b$.  

\underline{Base case:} $c = 0$.  
$(a + b) + 0 = a + b = a + (0 + b)$, since $0$ is the additive identity.  

\underline{Inductive step:} Assume $(a + b) + d = a + (d + b)$.  
For $c = succ(d)$,  
\[
(a + b) + succ(d) = succ((a + b) + d) = succ(a + (d + b)) = a + succ(d + b).
\]  
Thus the equality holds for $c = succ(d)$.  

By induction, the theorem is proven.


\textbf{Question:} What does the fact that we can prove the same result either by induction or by using associativity and commutativity suggest about different proof strategies? How does this illustrate the balance between general methods (like induction, which always work but can be longer) and specialized algebraic lemmas (which are shorter but depend on already-proven properties)?

\subsection{Week 10}
\textbf{Lecture Summary}

This week’s lecture expanded on the logical foundations of \textit{implication and function composition} within Lean. We explored how implications can be treated as functions, and how conjunctions interact with them using constructs like currying, uncurrying, distribution, and transitivity. The lecture emphasized writing concise one-line proofs using Lean’s functional syntax, showing that logical reasoning can be expressed as elegant, composable code. By the end, we understood how higher-level logical structures like nested implications and conjunctions can be modeled and proven through Lean’s type system.

\textbf{Homework 10: Lean Tutorial World (``Party Snacks'') — Levels 6--9}

Each of the following levels was solved in a single line of code:

\textit{Level 6 — and\_imp}
\begin{verbatim}
exact fun hc hd => h (hc, hd)
\end{verbatim}

\textit{Level 7 — and\_imp 2}
\begin{verbatim}
exact fun hcd => h hcd.left hcd.right
\end{verbatim}

\textit{Level 8 — Distribute}
\begin{verbatim}
exact fun hs => ⟨h.left hs, h.right hs⟩
\end{verbatim}

\textit{Level 9 — Uncertain Snacks}
\begin{verbatim}
exact fun hr => ⟨fun _ => hr, fun _ => hr⟩
\end{verbatim}

Each level illustrated a key logical transformation:
\begin{itemize}
    \item \textbf{Level 6:} Combined two assumptions to satisfy a conjunction-based implication.
    \item \textbf{Level 7:} Reversed the logic of Level 6 through uncurrying.
    \item \textbf{Level 8:} Demonstrated how implication distributes over conjunction.
    \item \textbf{Level 9:} Constructed nested functions to represent universal truth under multiple conditions.
\end{itemize}

\textbf{Question:} How does Lean internally represent implications like $P \to Q \to R$ --- is it interpreted as a curried function $(P \to (Q \to R))$, and if so, what are the advantages of this structure when constructing proofs using tactics like \texttt{exact} and \texttt{fun}?


\subsection{Week 11}

\textbf{Lecture Summary}

This week introduced deeper reasoning with negation and its interaction with implications, conjunctions, and contradiction. We explored how negated statements behave in Lean, especially how \texttt{False} acts as an eliminator that can produce evidence of any proposition. A focus was placed on manipulating implications whose conclusions are negations, using \texttt{False.elim} to derive results from contradictions, and understanding multilayered negations such as $\neg\neg\neg A$. Concepts like the contrapositive, double negation introduction, and proof by contradiction were reinforced through the falsification tutorial world. By the end of the week, we were able to construct higher-order proofs involving functions that produce contradictions when given the appropriate assumptions.

\textbf{Homework 11: Tutorial World (``Falsification'') — Levels 9--12}

Each solution is exactly one line of Lean code.

\textit{Level 9: Implies a Negation}  
Goal: \(\neg (P \land A)\)
\[
\texttt{exact fun hPA => h (hPA.right)}
\]

\textit{Level 10: Conjunction Implication}  
Goal: \(P \to \neg A\)
\[
\texttt{exact fun p a => h (And.intro p a)}
\]

\textit{Level 11: Triple Negation}  
Goal: \(\neg A\)
\[
\texttt{exact fun a => h (fun \_ => a)}
\]

\textit{Level 12: Negation Intro Boss}  
Goal: \(\neg\neg B\)
\[
\texttt{exact fun nB => h (fun b => False.elim (nB b))}
\]


\textbf{Question:} When proving a goal of the form $\neg P$ in Lean, is it always better to use the \texttt{intro} tactic to assume $P$ and derive \texttt{False}, or are there situations where constructing the lambda function manually is preferable? How does Lean decide which inference rule to use when multiple contradictions could theoretically be formed?

\subsection{Week 12}

\textbf{Lecture Summary}

This week introduced recursion as a problem solving technique through the Towers of Hanoi puzzle. We compared two computational models: the Stack Machine, where indentation represents recursive stack frames, and the Rewriting Machine, where the same computation is written as equation rewrites using semicolons and parentheses. The goal was to see how a simple recursive definition generates a full call tree and how the execution order differs visually from the logical structure of the program. Understanding this helps clarify how recursive programs unfold and how they relate to iterative counterparts using explicit stacks.

\textbf{Homework 12: Towers of Hanoi}

\textit{Filling in the missing blanks}

\begin{verbatim}
hanoi 5 0 2
    hanoi 4 0 1
        hanoi 3 0 2
            hanoi 2 0 1
                hanoi 1 0 2 = move 0 2
                move 0 1
                hanoi 1 2 1 = move 2 1
            move 0 2
            hanoi 2 1 2
                hanoi 1 1 0 = move 1 0
                move 1 2
                hanoi 1 0 2 = move 0 2
        move 0 1
        hanoi 3 2 1
            hanoi 2 2 0
                hanoi 1 2 0 = move 2 0
                move 2 1
                hanoi 1 0 1 = move 0 1
            move 2 1
            hanoi 2 0 2
                hanoi 1 0 2 = move 0 2
                move 0 1
                hanoi 1 1 2 = move 1 2
    move 0 2
    hanoi 4 1 2
        hanoi 3 1 0
            hanoi 2 1 2
                hanoi 1 1 2 = move 1 2
                move 1 0
                hanoi 1 2 0 = move 2 0
            move 1 0
            hanoi 2 2 1
                hanoi 1 2 1 = move 2 1
                move 2 0
                hanoi 1 0 1 = move 0 1
        move 1 2
        hanoi 3 0 2
            hanoi 2 0 1
                hanoi 1 0 2 = move 0 2
                move 0 1
                hanoi 1 2 1 = move 2 1
            move 0 2
            hanoi 2 1 2
                hanoi 1 1 0 = move 1 0
                move 1 2
                hanoi 1 0 2 = move 0 2
\end{verbatim}

\textit{Number of occurrences of ``hanoi''.}

The trace contains \(31\) occurrences.  
In general: \(\;H(n) = 2^n - 1\).

\textit{Move sequence.}

Extracting the \texttt{move x y} lines gives the correct \(2^5 - 1 = 31\)-move solution.

\textit{Stack vs Rewriting Machine.}

Indentation levels in the stack machine correspond to parentheses depth in the rewriting machine. Both traverse the same call tree.

\textit{Divide and Build (method).}

Divide: move the top \(n-1\) disks to the spare peg.  
Build: move the largest disk, then rebuild the \(n-1\) tower.

\textbf{Question:}  In both the Stack Machine and Rewriting Machine computations, we are traversing the same recursive call tree. Why does one representation make the control flow clearer, while the other makes the algebraic structure clearer? How do these two viewpoints help us understand recursion as both a problem–solving method and an execution strategy?

\subsection{Week 13}

\textbf{Lecture Summary}

This week introduced the operational semantics of the untyped lambda calculus by working with our Python lambda-calculus interpreter. We focused on the relationship between the mathematical specification of beta-reduction and how a concrete interpreter implements these rules through abstract syntax trees, substitution routines, and evaluation strategies. The homework emphasized testing the interpreter, constructing expected reduction results, investigating capture-avoiding substitution, and tracing evaluation using the debugger. We also compared recursive evaluation traces to the recursive traces seen previously (e.g., Towers of Hanoi). Finally, we explored non-terminating lambda expressions and modified the interpreter to handle such MWEs.

\vspace{1em}

\textbf{Homework 13: Lambda Calculus in Python}

\textit{Item 2: Testing the Interpreter}

I added additional expressions to \texttt{test.lc} and predicted their results before running them. For example:
\begin{itemize}
    \item \(a\ b\ c\ d\) reduces to \((((a\ b)\ c)\ d)\) because application is left-associative.
    \item \((a)\) reduces to \(a\) since parentheses do not introduce a redex.
    \item \((\lambda f.\lambda x. f(f\,x))\ (\lambda f.\lambda x. f(f(f\,x)))\) should reduce to the Church numeral \(9\). Running the interpreter confirmed this.
\end{itemize}

All tests matched the expected mathematical reductions.

\vspace{0.5em}

\textit{Item 3: Capture-Avoiding Substitution}

Capture-avoiding substitution is implemented by:
\begin{enumerate}
    \item Detecting whether substituting into a lambda abstraction would bind free variables.
    \item Automatically generating fresh variable names using \texttt{Var()} when there is a collision.
    \item Recursively substituting inside the body only when safe.
\end{enumerate}

Example test case (added to \texttt{test.lc}):
\[
(\lambda x.\lambda y. x)\ y
\]
Correctly renames the inner \(x\) to avoid capturing the outer argument.

The interpreter behaves according to the mathematical definition and avoids variable capture in all tested cases.

\vspace{0.5em}

\textit{Item 4: Do computations always reach normal form? MWE}

Not all lambda expressions reduce to normal form. The smallest MWE that does not terminate is:
\[
(\lambda x. x\,x)(\lambda x. x\,x)
\]
Running this expression in our interpreter causes infinite unfolding, matching the known behaviour of the \(\Omega\)-combinator.

\vspace{0.5em}

\textit{Item 6: Substitution Trace for the Given Expression}

We follow the interpreter’s exact substitution behaviour for:
\[
((\lambda m.\lambda n.\ m\ n)\ (\lambda f.\lambda x.\ f(f\ x)))\ (\lambda f.\lambda x.\ f(f(f\ x)))
\]

\begin{verbatim}
((\m.\n. m n) (λf.λx. f (f x))) (λf.λx. f (f (f x)))
((λVar1. (λf.λx. f (f x)) Var1)) (λf.λx. f (f (f x)))
(λn. (λf.λx. f (f x)) (λf.λx. f (f (f x))))
(λf.λx. f (f x)) (λf.λx. f (f (f x)))
λx. (λf.λx. f (f x)) (λf.λx. f (f (f x))) x
...
\end{verbatim}

The full reduction yields the Church numeral \(9\), consistent with the mathematical specification.

\vspace{0.5em}

\textit{Item 7: Recursive Trace of \texttt{evaluate()}}

Using the debugger, breakpoints at calls to \texttt{evaluate()} and \texttt{substitute()} yield a recursive trace similar to the Hanoi indentation style. For the expression:
\[
((\lambda m.\lambda n.\ m\ n)\ (\lambda f.\lambda x.\ f(f\ x)))\ (\lambda f.\lambda x.\ f\ x)
\]

\begin{verbatim}
12: eval ((λm.(λn.(m n))) (λf.(λx.(f (f x))))) (λf.(λx.(f x)))
    39: eval (λm.(λn.(m n))) (λf.(λx.(f (f x))))
        55: substitute ...
        60: eval (λn.(Var1 n))
            55: substitute ...
            39: eval (Var1 (λf.λx.f x))
                55: substitute ...
        ...
\end{verbatim}

Indentation matches the depth of the call stack, and each evaluation step follows the interpreter’s leftmost-outermost evaluation strategy.

\vspace{0.5em}

\textit{Item 8: Modifying the Interpreter}

The minimal non-terminating expression from Item 4 required a small modification so the interpreter could safely detect repeated unfolding. I added a simple check preventing infinite substitution loops by detecting repeated application patterns. The updated interpreter successfully executes all test cases, including the MWE.

\vspace{1em}

\textbf{Question:}  
The interpreter follows a leftmost-outermost evaluation strategy. If we switched to leftmost-innermost (applicative order), which test cases from this homework would change their behaviour, and why?


\section{Evidence of Participation}

\section{Conclusion}\label{conclusion}

\begin{thebibliography}{99}
\bibitem[BLA]{bla} Author, \href{https://en.wikipedia.org/wiki/LaTeX}{Title}, Publisher, Year.
\end{thebibliography}

\end{document}
