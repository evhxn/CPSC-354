\documentclass{article}

\usepackage{tikz} 
\usetikzlibrary{automata, positioning, arrows} 

\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{color}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{float}
\usepackage{forest}

\usepackage[section]{placeins}
  \hypersetup{
    colorlinks = true,
    urlcolor = blue,
    linkcolor= blue,
    citecolor= blue,
    filecolor= blue,
    }
    
\usepackage{listings}
\usepackage[utf8]{inputenc}                                                    
\usepackage[T1]{fontenc}                                                       

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=haskell,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newtheoremstyle{theorem}
  {\topsep}   % ABOVESPACE
  {\topsep}   % BELOWSPACE
  {\itshape\/}  % BODYFONT
  {0pt}       % INDENT
  {\bfseries} % HEADFONT
  {.}         % HEADPUNCT
  {5pt plus 1pt minus 1pt} % HEADSPACE
  {}
\theoremstyle{plain} 
   \newtheorem{theorem}{Theorem}[section]
   \newtheorem{corollary}[theorem]{Corollary}
   \newtheorem{lemma}[theorem]{Lemma}
   \newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
   \newtheorem{definition}[theorem]{Definition}
   \newtheorem{example}[theorem]{Example}
\theoremstyle{remark}    
  \newtheorem{remark}[theorem]{Remark}

\title{CPSC-354 Report}
\author{Ethan Tapia  \\ Chapman University}

\date{\today} 

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\setcounter{tocdepth}{3}
\tableofcontents

\section{Introduction}\label{intro}
This report documents my learning throughout CPSC 354: Programming Languages and Computability. 
The course explores computation from a formal and mathematical perspective, emphasizing the 
structures, rules, and logical principles that underlie programming languages. Rather than focusing 
on building software directly, the course approaches programming through rewriting systems, 
invariants, context-free grammars, proof assistants, and the operational semantics of the 
lambda calculus. Each week introduced a new conceptual tool for understanding how computation 
works at a foundational level.

The report is organized chronologically. Section 2 presents weekly summaries of the material, 
including lecture highlights, homework solutions, and reflective ``interesting questions’’ that 
connect course topics to larger themes in computation. Section 3 synthesizes the key ideas of 
the course and relates them to my systems engineering internship, where many of these formal 
concepts appeared implicitly in logic design, verification, and system behavior analysis. Section 4 
provides evidence of participation, and Section 5 offers a final reflection on how the course fits 
into the broader landscape of software engineering.

Overall, this report aims to demonstrate not only mastery of the technical material, but also how 
formal reasoning, rewriting systems, and logical structure intersect with practical engineering 
experience. The course gave me new conceptual tools for understanding computation, and these 
tools continue to shape how I think about programming, correctness, and system design.

\section{Week by Week}\label{homework}

% Week 1 

\subsection{Week 1}

\textbf{Lecture Summary}

We introduced \emph{formal systems} and worked with Hofstadter’s MIU-system as a rule–based rewriting game.  
Alphabet: $\Sigma=\{M,I,U\}$.  
Axiom (start string): $MI$.  
Production rules:
\begin{enumerate}
    \item[\textbf{(R1)}] If a string ends in $I$, append $U$: $xI \Rightarrow xIU$.
    \item[\textbf{(R2)}] If a string is $Mx$, duplicate $x$: $Mx \Rightarrow Mxx$.
    \item[\textbf{(R3)}] Replace any $III$ by $U$: $xIIIy \Rightarrow xUy$.
    \item[\textbf{(R4)}] Delete any $UU$: $xUUy \Rightarrow xy$.
\end{enumerate}
Key idea: reason about \emph{invariants} that rules preserve, instead of searching blindly through derivations.

\bigskip
\textbf{Homework: The MU-puzzle}

\begin{definition}[I–count and residue]
For a string $w$, let $\#_I(w)$ be the number of $I$’s in $w$, and define the residue
\[
\varphi(w) \;=\; \#_I(w) \bmod 3 \in \{0,1,2\}.
\]
\end{definition}

\begin{lemma}[Effect of each rule on $\#_I$]\label{lem:rule-effects}
For any string $w$:
\begin{enumerate}
    \item \textbf{(R1)} and \textbf{(R4)} do not change $\#_I$.
    \item \textbf{(R2)} doubles the number of $I$’s \emph{after} the initial $M$, so $\varphi$ is multiplied by $2$ modulo $3$.
    \item \textbf{(R3)} decreases $\#_I$ by $3$, so $\varphi$ is unchanged.
\end{enumerate}
\end{lemma}

\begin{proposition}[Invariant modulo $3$]\label{prop:invariant}
Every string derivable from $MI$ has $\varphi\in\{1,2\}$. In particular, no derivable string has $\varphi=0$.
\end{proposition}

\begin{proof}
We use induction on the length of a derivation from $MI$.

\emph{Base.} $\varphi(MI)=1$.

\emph{Step.} Assume $\varphi\in\{1,2\}$ for some derivable $w$.  
By Lemma~\ref{lem:rule-effects}, rules (R1), (R3), and (R4) keep $\varphi$ unchanged, and rule (R2) maps $1\leftrightarrow 2$ modulo $3$. None of these operations yields $0$ from a value in $\{1,2\}$. Therefore the next string also has $\varphi\in\{1,2\}$.
\end{proof}

\begin{theorem}[MU is unreachable]
\label{thm:mu-unreachable}
$MU$ cannot be derived from $MI$ in the MIU-system.
\end{theorem}

\begin{proof}
$MU$ contains zero $I$’s, hence $\varphi(MU)=0$. By Proposition~\ref{prop:invariant}, every derivable string has residue $1$ or $2$. Thus $MU$ is not derivable.
\end{proof}

\textit{Conclusion.} Starting from $MI$ we can toggle the residue $1\leftrightarrow 2$ with (R2) and otherwise keep it fixed with (R1), (R3), (R4). We never reach residue $0$, so no sequence of legal rule applications yields $MU$.

\begin{center}
\begin{tikzpicture}[node distance=3cm, auto]
  \tikzstyle{state}=[circle,draw,minimum size=9mm]
  \node[state, label=below:{residue of $\#I$}] (Z1) {1};
  \node[state, right=of Z1] (Z2) {2};
  \node[state, right=of Z2] (Z0) {0};
  \path[->, thick]
    (Z1) edge[bend left] node[above]{(R2)} (Z2)
    (Z2) edge[bend left] node[below]{(R2)} (Z1);
  \path[->, dashed]
    (Z1) edge[loop above] node{R1,R3,R4} ()
    (Z2) edge[loop above] node{R1,R3,R4} ()
    (Z0) edge[loop above] node{R1,R3,R4} ();

\end{tikzpicture}
\end{center}
\textbf{Question:} If the MU-puzzle shows that some goals are unreachable due to invariants (like the mod-3 property of I’s), how does this idea connect to undecidability in programming languages?

\FloatBarrier

% Week 2 
\subsection{Week 2}

\textbf{Lecture Summary} \\
We introduced \emph{Abstract Reduction Systems (ARS)}: a pair $(A,R)$ with one-step reduction $R\subseteq A\times A$. Key notions:
reducible/normal form, joinability, confluence, termination, and unique normal forms.

\bigskip
\textbf{Homework Part 2: The 8 Combinations}

We provide an example ARS for each combination of
$(\text{confluent}, \text{terminating}, \text{unique NFs})$.
If a row is impossible, we explain why.

\begin{center}
\begin{tabular}{|c|c|c|l|}
\hline
\textbf{Confluent} & \textbf{Terminating} & \textbf{Unique NFs} & \textbf{Example} \\
\hline
True  & True  & True  & $A=\{a\},\ R=\emptyset$ (Fig.~\ref{fig:combo-ttt}) \\
True  & True  & False & \emph{Impossible} \\
True  & False & True  & $A=\{a,b\},\ R=\{(a,a),(a,b)\}$ (Fig.~\ref{fig:combo-tft}) \\
True  & False & False & $A=\{a\},\ R=\{(a,a)\}$ (Fig.~\ref{fig:combo-tff}) \\
False & True  & True  & \emph{Impossible} \\
False & True  & False & $A=\{a,b,c\},\ R=\{(a,b),(a,c)\}$ (Fig.~\ref{fig:combo-ftf}) \\
False & False & True  & \emph{Impossible} \\
False & False & False & $A=\{a,b,c\},\ R=\{(a,b),(a,c),(b,b),(c,c)\}$ (Fig.~\ref{fig:combo-fff}) \\
\hline
\end{tabular}
\end{center}

\noindent\textit{Why some rows are impossible.}
If an ARS has unique normal forms, it must be confluent.
If an ARS is both confluent and terminating, then every element reduces to a unique normal form.
Therefore the rows \((\text{T},\text{T},\text{F})\), \((\text{F},\text{T},\text{T})\), and \((\text{F},\text{F},\text{T})\) cannot occur.

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
\end{tikzpicture}
\caption{Combination (True, True, True). Terminating, confluent, unique NF.}
\label{fig:combo-ttt}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
  \node[obj] (b) [right=of a] {$b$};
  \path (a) edge (b)
        (a) edge[loop above] (a);
\end{tikzpicture}
\caption{Combination (True, False, True). Non-terminating, confluent, unique NF $b$.}
\label{fig:combo-tft}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
  \path (a) edge[loop right] (a);
\end{tikzpicture}
\caption{Combination (True, False, False). Non-terminating, confluent, no normal form.}
\label{fig:combo-tff}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3.2cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
  \node[obj] (b) [right=of a,yshift=10pt] {$b$};
  \node[obj] (c) [right=of a,yshift=-10pt] {$c$};
  \path (a) edge (b)
        (a) edge (c);
\end{tikzpicture}
\caption{Combination (False, True, False). Terminating, not confluent; two distinct normal forms $b,c$ are not joinable.}
\label{fig:combo-ftf}
`\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',node distance=3.2cm,auto]
  \tikzstyle{obj}=[circle,draw,minimum size=9mm]
  \node[obj] (a) {$a$};
  \node[obj] (b) [right=of a,yshift=10pt] {$b$};
  \node[obj] (c) [right=of a,yshift=-10pt] {$c$};
  \path (a) edge (b)
        (a) edge (c)
        (b) edge[loop right] (b)
        (c) edge[loop right] (c);
\end{tikzpicture}
\caption{Combination (False, False, False). Non-terminating (loops), not confluent, no unique normal forms.}
\label{fig:combo-fff}
\end{figure}

\FloatBarrier

\noindent\paragraph{Conclusion.}
The MU-puzzle illustrates how invariants prove impossibility in a formal system.
The ARS framework provides the general language to study rewrite systems via termination, confluence, and normal forms.
The 8-combination analysis shows which behaviors are possible and which are structurally impossible.

\noindent\paragraph{Question:}
Could there be a general framework that unifies invariants with confluence and termination, so that impossibility and determinism appear as two sides of the same rewriting theory?

% Week 3
\subsection{Week 3}
\textbf{Lecture Summary}
\\This lecture expanded on string- and term-rewriting techniques and connected the abstract theory of
reduction systems to practical methods for reasoning about equivalence and normal forms. We
reviewed the key semantic properties of an abstract reduction system (termination, confluence,
unique normal forms) and introduced tools for proving or disproving them in concrete string
rewriting examples. Important results covered include the Church--Rosser / Diamond intuition and
Newman's lemma (termination plus local confluence implies confluence), and the role of critical
pairs and joinability when analysing overlapping rewrite rules. The lecture also emphasised
strategies for finding normal forms (e.g. orienting rules to make a system terminating) and for
using invariants to prove impossibility or non-equivalence; this prepared us for Homework 3,
where students experiment with small string rewrite systems and classify equivalence classes by
their normal forms.

\textbf{Homework 3}
\paragraph{Exercise 5}

Consider an ARS with
\[
A = \{a,b\}^* = \{\varepsilon, a, b, aa, ab, ba, bb, aaa, \dots \}
\]
and rewrite rules
\[
ab \to ba, \qquad 
ba \to ab, \qquad 
aa \to \varepsilon, \qquad 
b \to \varepsilon
\]

\begin{enumerate}
  \item \textbf{Reduce some example strings such as $abba$ and $bababa$.}
  \[
    abba \to aa \to \varepsilon, \qquad
    bababa \to aaa \to a
  \]

  \item \textbf{Find two strings that are not equivalent. How many non-equivalent strings can you find?}
  \begin{itemize}
    \item $\varepsilon$
    \item $a$
  \end{itemize}
  These have different normal forms and cannot be transformed into each other.

  \item \textbf{How many equivalence classes does $\stackrel{\ast}{\longleftrightarrow}$ have? What are the normal forms?} \\
  There are two equivalence classes:
  \begin{enumerate}
    \item Strings whose normal form is $\varepsilon$,
    \item Strings whose normal form is $a$.
  \end{enumerate}
  The class is determined by the parity of the number of $a$’s in the string.

  \item \textbf{Can you modify the ARS so that it becomes terminating without changing its equivalence classes?} \\
  Yes. Remove one of the first two rules. They only permute $a$ and $b$ and do not affect equivalence classes, but having both makes the system non-terminating.

  \item \textbf{Question:} \\
  If I remove all the $b$’s from a string, does the remaining word reduce to $a$ or to $\varepsilon$?  

  \textbf{Answer:} This can be answered using the ARS because $b \to \varepsilon$ always deletes $b$’s, and the final result depends only on whether the number of $a$’s left is odd or even. Odd $\mapsto a$, even $\mapsto \varepsilon$.
\end{enumerate}

\paragraph{Exercise 5b}

Now replace the rule $aa \to \varepsilon$ with $aa \to a$.

\begin{enumerate}
  \item \textbf{Reduce some example strings such as $abba$ and $bababa$.}
  \[
    abba \to aa \to a, \qquad
    bababa \to aaa \to aa \to a
  \]

  \item \textbf{Find two strings that are not equivalent.}
  \begin{itemize}
    \item $\varepsilon$
    \item $a$
  \end{itemize}

  \item \textbf{How many equivalence classes are there? What are the normal forms?} \\
  There are two equivalence classes:
  \begin{enumerate}
    \item Strings with no $a$’s $\mapsto$ normal form $\varepsilon$,
    \item Strings with at least one $a$ $\mapsto$ normal form $a$.
  \end{enumerate}

  \item \textbf{Modify the ARS to make it terminating.} \\
  As above, remove one of the two swapping rules $ab \leftrightarrow ba$.

  \item \textbf{Question:} \\
  Is the system confluent? That is, if a string can be reduced in two different ways, do the reductions always lead to the same normal form?
\end{enumerate}

% Week 4
\subsection{Week 4}
\textbf{Lecture Summary}
\\An \emph{invariant} is a function or property that remains unchanged under the rewriting relation of an ARS. 
They are central tools across science (e.g.\ conservation laws in physics, chemistry, and biology) and mathematics. 
Formally, $P:A\to B$ is an invariant if $a\to b \Rightarrow P(a)=P(b)$. 
Strong invariants preserve exact equality, while weak invariants preserve truth of properties. 
Invariants induce partitions on $A$, often serving as abstractions of the equivalence relation $\leftrightarrow^\ast$. 
They can be used to prove impossibility (show $P(a)=\text{true}$, $P(b)=\text{false}$) and to build \emph{complete invariants}, which fully classify equivalence classes. 
Examples include letter counts in string rewriting systems and parity arguments in puzzles (domino tilings, sliding puzzles). 
In programming, invariants explain correctness of while-loops and recursion, while measure functions guarantee termination.


\textbf{Homework 4.1}\\
\textbf{Algorithm}
\begin{verbatim}
while b != 0:
  temp = b
  b = a mod b
  a = temp
return a
\end{verbatim}

\paragraph{Conditions under which it always terminates.}
Assume \(a,b\in\mathbb{N}\) with \(b\ge 0\). If \(b=0\) the loop does not run and the program returns immediately. If \(b>0\) then each loop iteration is well defined and yields a strictly smaller nonnegative \(b\) because \(a \bmod b\in\{0,1,\dots,b-1\}\). Thus the loop must terminate. (Equivalently: Euclid’s algorithm terminates for all nonnegative integers, not both zero.)

\paragraph{Measure function and proof.}
Let the state be the pair \((a,b)\in\mathbb{N}^2\). Define
\[
\phi(a,b)=b.
\]
Suppose the guard holds, so \(b>0\). One loop step computes
\[
(a',b')=(b,\; a\bmod b).
\]
Then \(0\le b' < b\), hence \(\phi(a',b')=b' < b=\phi(a,b)\).
Therefore \(\phi\) strictly decreases on every iteration while staying in \(\mathbb{N}\). Since \(>\) on \(\mathbb{N}\) is well founded, no infinite descent exists, so the loop terminates.

\textbf{Homework 4.2}\\
\textbf{Fragment}
\begin{verbatim}
function merge_sort(arr, left, right):
  if left >= right:
    return
  mid = (left + right) / 2   // integer division
  merge_sort(arr, left, mid)
  merge_sort(arr, mid+1, right)
  merge(arr, left, mid, right)
\end{verbatim}

\paragraph{Claim.}
\(\displaystyle \phi(left,right)=right-left+1\) is a measure function for the recursive calls of \texttt{merge\_sort}.

\paragraph{Proof.}
We reason about the domain \(D=\{(l,r)\in\mathbb{Z}^2 \mid l\le r\}\) with the measure \(\phi(l,r)=r-l+1\in\mathbb{N}\).

If \(left\ge right\) then \(\phi(left,right)\in\{0,1\}\) and the function returns, so there is no recursive descent.

Assume \(left<right\). Let \(mid=\lfloor (left+right)/2\rfloor\). Standard bounds give
\[
left \le mid < right \quad\text{and}\quad left < mid+1 \le right.
\]
Hence both subranges are valid:
\[
(left,mid)\in D, \qquad (mid+1,right)\in D.
\]
Their measures satisfy
\[
\phi(left,mid)=mid-left+1 \le \left\lfloor\frac{left+right}{2}\right\rfloor-left+1
< \frac{left+right}{2}-left+1
= \frac{right-left+2}{2} \le right-left,
\]
so \(\phi(left,mid) \le right-left < right-left+1=\phi(left,right)\). Similarly,
\[
\phi(mid+1,right)=right-(mid+1)+1=right-mid
\le right-\left\lfloor\frac{left+right}{2}\right\rfloor
< right-\frac{left+right}{2}
= \frac{right-left}{2}
< right-left+1=\phi(left,right).
\]
Thus each recursive argument strictly decreases the measure \(\phi\). Since \(\phi\) takes values in \(\mathbb{N}\) and strictly decreases along every recursion chain, the recursion is well founded and \texttt{merge\_sort} terminates.

\textbf{Question:} \\
We can discovered that Euclid’s algorithm always stops. But how could you use an invariant to also show that it actually gives the greatest common divisor, not just any number?


% Week 5
\subsection{Week 5}
\textbf{Lecture Summary}\\
Lambda calculus is a minimal but Turing-complete language with only three constructs: abstraction ($\lambda x.e$ defines a function), application ($e_1\,e_2$ applies a function to an argument), and variables (simple names without assignment). Application associates to the left and abstraction chains naturally. Computation is substitution: $(\lambda x.M)\,N \rightsquigarrow M[N/x]$ (the $\beta$-rule), with bound variables freely renamable ($\alpha$-equivalence) to avoid capture. Functions can return functions (currying), and using Church encodings, numbers and arithmetic can be represented purely by substitution.


\textbf{Homework 5: Lambda Calculus Reduction}\\
\textbf{We Evaluate:}
\[
(\lambda f.\,\lambda x.\, f(f(x))) \; (\lambda f.\,\lambda x.\, f(f(f(x))))
\]
\noindent
\textbf{Step 1: Rename the boud variables of the second term to avoid clashes} 
\[
(\lambda f.\,\lambda x.\, f(f(x))) \; (\lambda g.\,\lambda y.\, g(g(g(y))))
\]

\noindent
\textbf{Step 2: Apply the outer function to its argument}
\[
\lambda x.\, (\lambda g.\,\lambda y.\, g(g(g(y)))) \big( (\lambda g.\,\lambda y.\, g(g(g(y))))\, x \big)
\]

\noindent
\textbf{Step 3: Reduce the inner application}
\[
\lambda x.\, (\lambda g.\,\lambda y.\, g(g(g(y)))) \, (\lambda y.\, x(x(xy)))
\]

\noindent
\textbf{Step 4: Apply again}
\[
\lambda x.\,\lambda y.\, (\lambda y.\, x(x(xy))) \big((\lambda y.\, x(x(xy))) \, ((\lambda y.\, x(x(xy)))\, y)\big)
\]

\noindent
\textbf{Step 5: Evaluate the nested calls}
\[
\lambda x.\,\lambda y.\, x(x(x(x(x(x(x(x(xy)))))))))
\]

\noindent
\textbf{Final result.} This is the Church numeral
\[
\lambda f.\,\lambda x.\, f^9(x)
\]
This is the number $9$ in Church encoding.

\medskip
\noindent
\emph{Note:} The workout shows that $2\,3 = 9$ for Church numerals. In general, Church numerals encode repeated function application, and application corresponds to multiplication.

\textbf{Question:} If variable names don’t matter in $\lambda$
-calculus, what does that suggest about how meaning can exist independently of representation?

% Week 6
\subsection{Week 6}

\textbf{Lecture Summary}\\  
This lecture introduced recursion in the $\lambda$-calculus via the \textit{fixed point combinator}. We learned that recursion can be encoded without special syntax by defining \texttt{fix} such that $\texttt{fix}\ F \to F(\texttt{fix}\ F)$. Using this, one can define recursive functions like factorial. We also reviewed the definitions of \texttt{let} and \texttt{let rec}, which expand into $\lambda$-abstractions and applications of \texttt{fix}. The key point is that recursion in functional languages comes from self-application and fixed points, with the famous $Y$-combinator as a canonical construction.

\textbf{Homework 6: Fixed Points and Recursion}\\

\textbf{Rules:}\\
\[
\begin{aligned}
\texttt{fix}\ F &\;\to\; F\,(\texttt{fix}\ F) \qquad &\textbf{(def of fix)}\\
\texttt{let}\ x=e_1\ \texttt{in}\ e_2 &\;\to\; (\lambda x.\,e_2)\ e_1 \qquad &\textbf{(def of let)}\\
\texttt{let rec}\ f=e_1\ \texttt{in}\ e_2 &\;\to\; \texttt{let}\ f=(\texttt{fix}\ (\lambda f.\,e_1))\ \texttt{in}\ e_2 \qquad &\textbf{(def of let rec)}
\end{aligned}
\]

\textbf{Abbreviation.}  
For readability set
\[
G \;\equiv\; \lambda f.\,\lambda n.\, \texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n * f(n-1).
\]

\textbf{Goal term.}
\[
\texttt{let rec}\ \texttt{fact} = \lambda n.\, \texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n * \texttt{fact}(n-1)\ \texttt{in}\ \texttt{fact}\ 3
\]

\textbf{Derivation}
\[
\begin{aligned}
&\texttt{let rec}\ \texttt{fact} = \lambda n.\,\texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n * \texttt{fact}(n-1)\ \texttt{in}\ \texttt{fact}\ 3
\\
\to\;& \texttt{let}\ \texttt{fact} = \texttt{fix}\,G\ \texttt{in}\ \texttt{fact}\ 3 \quad \textbf{\small<def of let rec>}
\\
\to\;& (\lambda \texttt{fact}.\, \texttt{fact}\ 3)\ (\texttt{fix}\,G) \quad \textbf{\small<def of let>}
\\
\to\;& (\texttt{fix}\,G)\ 3 \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& (G(\texttt{fix}\,G))\ 3 \quad \textbf{\small<def of fix>}
\\
\to\;& (\lambda n.\, \texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n*(\texttt{fix}\,G)(n-1))\ 3 \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& \texttt{if}\ 3=0\ \texttt{then}\ 1\ \texttt{else}\ 3 * (\texttt{fix}\,G)(2) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (\texttt{fix}\,G)(2) \quad \textbf{\small<def of if>}
\\
\to\;& 3 * \big(G(\texttt{fix}\,G)\big)\ 2 \quad \textbf{\small<def of fix>}
\\
\to\;& 3 * (\lambda n.\,\texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n*(\texttt{fix}\,G)(n-1))\ 2 \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (\texttt{if}\ 2=0\ \texttt{then}\ 1\ \texttt{else}\ 2*(\texttt{fix}\,G)(1)) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (\texttt{fix}\,G)(1)) \quad \textbf{\small<def of if>}
\\
\to\;& 3 * (2 * (G(\texttt{fix}\,G))\ 1) \quad \textbf{\small<def of fix>}
\\
\to\;& 3 * (2 * (\lambda n.\,\texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n*(\texttt{fix}\,G)(n-1))\ 1) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (\texttt{if}\ 1=0\ \texttt{then}\ 1\ \texttt{else}\ 1*(\texttt{fix}\,G)(0))) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (1 * (\texttt{fix}\,G)(0))) \quad \textbf{\small<def of if>}
\\
\to\;& 3 * (2 * (1 * (G(\texttt{fix}\,G))\ 0)) \quad \textbf{\small<def of fix>}
\\
\to\;& 3 * (2 * (1 * (\lambda n.\,\texttt{if}\ n=0\ \texttt{then}\ 1\ \texttt{else}\ n*(\texttt{fix}\,G)(n-1))\ 0)) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (1 * (\texttt{if}\ 0=0\ \texttt{then}\ 1\ \texttt{else}\ 0*(\texttt{fix}\,G)(-1)))) \quad \textbf{\small<}\beta\textbf{-rule>}
\\
\to\;& 3 * (2 * (1 * 1)) \quad \textbf{\small<def of if>}
\\
\to\;& 3 * (2 * 1) \quad \textbf{\small<arith>}
\\
\to\;& 3 * 2 \quad \textbf{\small<arith>}
\\
\to\;& 6 \quad \textbf{\small<arith>}
\end{aligned}
\]

\textbf{Result:} \texttt{fact 3} reduces to $6$, each step justified by \textbf{def of let rec}, \textbf{def of let}, \(\beta\)\textbf{-rule}, \textbf{def of fix}, \textbf{def of if}, and \textbf{arith}.

\textbf{Question:} Since the fixed point combinator allows functions to call themselves without being named, what does this suggest about the nature of recursion and whether naming is essential for defining self-reference?

% Week 7
\subsection{Week 7}\\
\textbf{Lecture Summary}\\
This lecture introduced parsing and context-free grammars (CFGs) using the calculator example. Parsing was explained as the process of turning concrete syntax (strings) into abstract syntax (trees). We saw how CFG rules capture precedence and associativity, and how parse trees differ from simplified abstract syntax trees (ASTs). Lisp was discussed as a language where programmers essentially write abstract syntax directly. \\

\textbf{Homework: Parse Trees for Arithmetic Expressions}

\textbf{1. Expression: $2+1$}

\begin{forest}
[Exp
  [Exp
    [Exp1
      [Exp2
        [Integer [2]]]]]
  [+]
  [Exp1
    [Exp2
      [Integer [1]]]]]
\end{forest}

\textbf{2. Expression: $1+2*3$}

\begin{forest}
[Exp
  [Exp
    [Exp1
      [Exp2
        [Integer [1]]]]]
  [+]
  [Exp1
    [Exp1
      [Exp2 [Integer [2]]]]
    [*]
    [Exp2 [Integer [3]]]]]
\end{forest}

\textbf{3. Expression: $1+(2*3)$}

\begin{forest}
[Exp
  [Exp
    [Exp1
      [Exp2
        [Integer [1]]]]]
  [+]
  [Exp1
    [Exp2
      [(]
        [Exp
          [Exp1
            [Exp1
              [Exp2 [Integer [2]]]]
            [*]
            [Exp2 [Integer [3]]]]]
      [)]]]]
\end{forest}

\textbf{4. Expression: $(1+2)*3$}

\begin{forest}
[Exp
  [Exp1
    [Exp1
      [Exp2
        [(]
          [Exp
            [Exp
              [Exp1
                [Exp2 [Integer [1]]]]]
            [+]
            [Exp1
              [Exp2 [Integer [2]]]]]
        [)]]]
    [*]
    [Exp2 [Integer [3]]]]]
\end{forest}


\textbf{5. Expression: $1+2*3+4*5+6$}

\begin{forest}
[Exp
  [Exp
    [Exp
      [Exp1 [Exp2 [Integer [1]]]]
      [+]
      [Exp1
        [Exp1 [Exp2 [Integer [2]]]]
        [*]
        [Exp2 [Integer [3]]]]]
    [+]
    [Exp1
      [Exp1 [Exp2 [Integer [4]]]]
      [*]
      [Exp2 [Integer [5]]]]]
  [+]
  [Exp1
    [Exp2 [Integer [6]]]]]
\end{forest}

\textbf{Question:} If Lisp lets programmers write abstract syntax directly, what does this reveal about the trade-offs between readability for humans and ease of parsing for machines?

% Week 8
\subsection{Week 8}
\textbf{Lecture Summary}

This lecture introduced formal proofs in Lean using the natural numbers tutorial world. We practiced rewriting with lemmas such as \texttt{add\_zero} and \texttt{add\_succ}, and learned how to control which occurrence gets rewritten. The exercises illustrated how even simple arithmetic like $2 + 2 = 4$ requires careful step-by-step rewriting when starting from the axioms of Peano arithmetic. The key insight is that Lean forces us to be explicit about each rule application, which deepens our understanding of how proofs are built from small definitional steps.

\textbf{Homework 8: Lean Tutorial World (Levels 5--8)}

\textit{Level 5.} Prove $a + (b + 0) + (c + 0) = a + b + c$.

\begin{verbatim}
rw [add_zero c]
rw [add_zero]
rfl
\end{verbatim}

\textit{Level 6.} Prove $succ n = n + 1$.

\begin{verbatim}
rw [one_eq_succ_zero]
rw [add_succ]
rw [add_zero]
rfl
\end{verbatim}

\textit{Level 7.} Prove $2 + 2 = 4$.

\begin{verbatim}
rw [two_eq_succ_one]
rw [add_succ]
rw [one_eq_succ_zero]
rw [add_succ]
rw [add_zero]
rw [four_eq_succ_three]
rw [three_eq_succ_two]
rw [two_eq_succ_one]
rw [one_eq_succ_zero]
rfl
\end{verbatim}

\textit{Level 8 (alternative short proof).} Another valid solution using \texttt{nth\_rewrite} and reversed rewrites.

\begin{verbatim}
nth_rewrite 2 [two_eq_succ_one]
rw [add_succ]
rw [one_eq_succ_zero]
rw [add_succ, add_zero]
rw [← three_eq_succ_two]
rw [← four_eq_succ_three]
rfl
\end{verbatim}

\textbf{Question:} When Lean forces us to spell out each small step (like showing $2 + 2 = 4$ from the Peano axioms), it reveals how much structure is hidden in even basic arithmetic. What does this suggest about the trade-off between human mathematical intuition (where $2 + 2 = 4$ is obvious) and machine-checked rigor (where nothing is obvious until proved)?

% Week 9
\subsection{Week 9}
\textbf{Lecture Summary}

This lecture focused on proving properties of addition using associativity and commutativity. We learned how Lean allows us to move brackets with \texttt{add\_assoc} and reorder terms with \texttt{add\_comm}, avoiding induction in many cases. At the same time, we also practiced writing an inductive proof, showing how different proof strategies can establish the same result. The key takeaway is that structural lemmas like associativity and commutativity make proofs shorter and more direct, but induction remains a powerful general method.

\textbf{Homework 9: Lean Tutorial World (Level 5)}

\textit{Goal:} Prove $(a + b) + c = a + (c + b)$.

---

\textit{Solution 1 (without induction).}

\begin{verbatim}
rw [add_assoc]
rw [add_comm b c]
rw [add_assoc]
rfl
\end{verbatim}

\textbf{Mathematical proof:}  
Starting with $(a + b) + c$, by associativity we have $a + (b + c)$.  
By commutativity of addition, this equals $a + (c + b)$.  
By associativity again, this is $(a + c) + b$, which proves the equality.

---

\textit{Solution 2 (with induction).}

\begin{verbatim}
induction c with d hd,
{ rw [add_zero], rw [add_zero], rfl },
{ rw [add_succ], rw [add_succ], rw hd, rfl }
\end{verbatim}

\textbf{Mathematical proof:}  
We prove by induction on $c$ that $(a + b) + c = a + (c + b)$ for all $a, b$.  

\underline{Base case:} $c = 0$.  
$(a + b) + 0 = a + b = a + (0 + b)$, since $0$ is the additive identity.  

\underline{Inductive step:} Assume $(a + b) + d = a + (d + b)$.  
For $c = succ(d)$,  
\[
(a + b) + succ(d) = succ((a + b) + d) = succ(a + (d + b)) = a + succ(d + b).
\]  
Thus the equality holds for $c = succ(d)$.  

By induction, the theorem is proven.


\textbf{Question:} What does the fact that we can prove the same result either by induction or by using associativity and commutativity suggest about different proof strategies? How does this illustrate the balance between general methods (like induction, which always work but can be longer) and specialized algebraic lemmas (which are shorter but depend on already-proven properties)?

% Week 10
\subsection{Week 10}
\textbf{Lecture Summary}

This week’s lecture expanded on the logical foundations of \textit{implication and function composition} within Lean. We explored how implications can be treated as functions, and how conjunctions interact with them using constructs like currying, uncurrying, distribution, and transitivity. The lecture emphasized writing concise one-line proofs using Lean’s functional syntax, showing that logical reasoning can be expressed as elegant, composable code. By the end, we understood how higher-level logical structures like nested implications and conjunctions can be modeled and proven through Lean’s type system.

\textbf{Homework 10: Lean Tutorial World (``Party Snacks'') — Levels 6--9}

Each of the following levels was solved in a single line of code:

\textit{Level 6 — and\_imp}
\begin{verbatim}
exact fun hc hd => h (hc, hd)
\end{verbatim}

\textit{Level 7 — and\_imp 2}
\begin{verbatim}
exact fun hcd => h hcd.left hcd.right
\end{verbatim}

\textit{Level 8 — Distribute}
\begin{verbatim}
exact fun hs => ⟨h.left hs, h.right hs⟩
\end{verbatim}

\textit{Level 9 — Uncertain Snacks}
\begin{verbatim}
exact fun hr => ⟨fun _ => hr, fun _ => hr⟩
\end{verbatim}

Each level illustrated a key logical transformation:
\begin{itemize}
    \item \textbf{Level 6:} Combined two assumptions to satisfy a conjunction-based implication.
    \item \textbf{Level 7:} Reversed the logic of Level 6 through uncurrying.
    \item \textbf{Level 8:} Demonstrated how implication distributes over conjunction.
    \item \textbf{Level 9:} Constructed nested functions to represent universal truth under multiple conditions.
\end{itemize}

\textbf{Question:} How does Lean internally represent implications like $P \to Q \to R$ --- is it interpreted as a curried function $(P \to (Q \to R))$, and if so, what are the advantages of this structure when constructing proofs using tactics like \texttt{exact} and \texttt{fun}?

% Week 11
\subsection{Week 11}

\textbf{Lecture Summary}

This week introduced deeper reasoning with negation and its interaction with implications, conjunctions, and contradiction. We explored how negated statements behave in Lean, especially how \texttt{False} acts as an eliminator that can produce evidence of any proposition. A focus was placed on manipulating implications whose conclusions are negations, using \texttt{False.elim} to derive results from contradictions, and understanding multilayered negations such as $\neg\neg\neg A$. Concepts like the contrapositive, double negation introduction, and proof by contradiction were reinforced through the falsification tutorial world. By the end of the week, we were able to construct higher-order proofs involving functions that produce contradictions when given the appropriate assumptions.

\textbf{Homework 11: Tutorial World (``Falsification'') — Levels 9--12}

Each solution is exactly one line of Lean code.

\textit{Level 9: Implies a Negation}  
Goal: \(\neg (P \land A)\)
\[
\texttt{exact fun hPA => h (hPA.right)}
\]

\textit{Level 10: Conjunction Implication}  
Goal: \(P \to \neg A\)
\[
\texttt{exact fun p a => h (And.intro p a)}
\]

\textit{Level 11: Triple Negation}  
Goal: \(\neg A\)
\[
\texttt{exact fun a => h (fun \_ => a)}
\]

\textit{Level 12: Negation Intro Boss}  
Goal: \(\neg\neg B\)
\[
\texttt{exact fun nB => h (fun b => False.elim (nB b))}
\]


\textbf{Question:} When proving a goal of the form $\neg P$ in Lean, is it always better to use the \texttt{intro} tactic to assume $P$ and derive \texttt{False}, or are there situations where constructing the lambda function manually is preferable? How does Lean decide which inference rule to use when multiple contradictions could theoretically be formed?

% Week 12
\subsection{Week 12}

\textbf{Lecture Summary}

This week introduced recursion as a problem solving technique through the Towers of Hanoi puzzle. We compared two computational models: the Stack Machine, where indentation represents recursive stack frames, and the Rewriting Machine, where the same computation is written as equation rewrites using semicolons and parentheses. The goal was to see how a simple recursive definition generates a full call tree and how the execution order differs visually from the logical structure of the program. Understanding this helps clarify how recursive programs unfold and how they relate to iterative counterparts using explicit stacks.

\textbf{Homework 12: Towers of Hanoi}

\textit{Filling in the missing blanks}

\begin{verbatim}
hanoi 5 0 2
    hanoi 4 0 1
        hanoi 3 0 2
            hanoi 2 0 1
                hanoi 1 0 2 = move 0 2
                move 0 1
                hanoi 1 2 1 = move 2 1
            move 0 2
            hanoi 2 1 2
                hanoi 1 1 0 = move 1 0
                move 1 2
                hanoi 1 0 2 = move 0 2
        move 0 1
        hanoi 3 2 1
            hanoi 2 2 0
                hanoi 1 2 0 = move 2 0
                move 2 1
                hanoi 1 0 1 = move 0 1
            move 2 1
            hanoi 2 0 2
                hanoi 1 0 2 = move 0 2
                move 0 1
                hanoi 1 1 2 = move 1 2
    move 0 2
    hanoi 4 1 2
        hanoi 3 1 0
            hanoi 2 1 2
                hanoi 1 1 2 = move 1 2
                move 1 0
                hanoi 1 2 0 = move 2 0
            move 1 0
            hanoi 2 2 1
                hanoi 1 2 1 = move 2 1
                move 2 0
                hanoi 1 0 1 = move 0 1
        move 1 2
        hanoi 3 0 2
            hanoi 2 0 1
                hanoi 1 0 2 = move 0 2
                move 0 1
                hanoi 1 2 1 = move 2 1
            move 0 2
            hanoi 2 1 2
                hanoi 1 1 0 = move 1 0
                move 1 2
                hanoi 1 0 2 = move 0 2
\end{verbatim}

\textit{Number of occurrences of ``hanoi''.}

The trace contains \(31\) occurrences.  
In general: \(\;H(n) = 2^n - 1\).

\textit{Move sequence.}

Extracting the \texttt{move x y} lines gives the correct \(2^5 - 1 = 31\)-move solution.

\textit{Stack vs Rewriting Machine.}

Indentation levels in the stack machine correspond to parentheses depth in the rewriting machine. Both traverse the same call tree.

\textit{Divide and Build (method).}

Divide: move the top \(n-1\) disks to the spare peg.  
Build: move the largest disk, then rebuild the \(n-1\) tower.

\textbf{Question:}  In both the Stack Machine and Rewriting Machine computations, we are traversing the same recursive call tree. Why does one representation make the control flow clearer, while the other makes the algebraic structure clearer? How do these two viewpoints help us understand recursion as both a problem–solving method and an execution strategy?

% Week 13
\subsection{Week 13}

\textbf{Lecture Summary}

This week introduced the operational semantics of the untyped lambda calculus by working with our Python lambda-calculus interpreter. We focused on the relationship between the mathematical specification of beta-reduction and how a concrete interpreter implements these rules through abstract syntax trees, substitution routines, and evaluation strategies. The homework emphasized testing the interpreter, constructing expected reduction results, investigating capture-avoiding substitution, and tracing evaluation using the debugger. We also compared recursive evaluation traces to the recursive traces seen previously (e.g., Towers of Hanoi). Finally, we explored non-terminating lambda expressions and modified the interpreter to handle such MWEs.

\vspace{1em}

\textbf{Homework 13: Lambda Calculus in Python}

\textit{Item 2: Testing the Interpreter}

I added additional expressions to \texttt{test.lc} and predicted their results before running them. For example:
\begin{itemize}
    \item \(a\ b\ c\ d\) reduces to \((((a\ b)\ c)\ d)\) because application is left-associative.
    \item \((a)\) reduces to \(a\) since parentheses do not introduce a redex.
    \item \((\lambda f.\lambda x. f(f\,x))\ (\lambda f.\lambda x. f(f(f\,x)))\) should reduce to the Church numeral \(9\). Running the interpreter confirmed this.
\end{itemize}

All tests matched the expected mathematical reductions.

\vspace{0.5em}

\textit{Item 3: Capture-Avoiding Substitution}

Capture-avoiding substitution is implemented by:
\begin{enumerate}
    \item Detecting whether substituting into a lambda abstraction would bind free variables.
    \item Automatically generating fresh variable names using \texttt{Var()} when there is a collision.
    \item Recursively substituting inside the body only when safe.
\end{enumerate}

Example test case (added to \texttt{test.lc}):
\[
(\lambda x.\lambda y. x)\ y
\]
Correctly renames the inner \(x\) to avoid capturing the outer argument.

The interpreter behaves according to the mathematical definition and avoids variable capture in all tested cases.

\vspace{0.5em}

\textit{Item 4: Do computations always reach normal form? MWE}

Not all lambda expressions reduce to normal form. The smallest MWE that does not terminate is:
\[
(\lambda x. x\,x)(\lambda x. x\,x)
\]
Running this expression in our interpreter causes infinite unfolding, matching the known behaviour of the \(\Omega\)-combinator.

\vspace{0.5em}

\textit{Item 6: Substitution Trace for the Given Expression}

We follow the interpreter’s exact substitution behaviour for:
\[
((\lambda m.\lambda n.\ m\ n)\ (\lambda f.\lambda x.\ f(f\ x)))\ (\lambda f.\lambda x.\ f(f(f\ x)))
\]

\begin{verbatim}
((\m.\n. m n) (λf.λx. f (f x))) (λf.λx. f (f (f x)))
((λVar1. (λf.λx. f (f x)) Var1)) (λf.λx. f (f (f x)))
(λn. (λf.λx. f (f x)) (λf.λx. f (f (f x))))
(λf.λx. f (f x)) (λf.λx. f (f (f x)))
λx. (λf.λx. f (f x)) (λf.λx. f (f (f x))) x
...
\end{verbatim}

The full reduction yields the Church numeral \(9\), consistent with the mathematical specification.

\vspace{0.5em}

\textit{Item 7: Recursive Trace of \texttt{evaluate()}}

Using the debugger, breakpoints at calls to \texttt{evaluate()} and \texttt{substitute()} yield a recursive trace similar to the Hanoi indentation style. For the expression:
\[
((\lambda m.\lambda n.\ m\ n)\ (\lambda f.\lambda x.\ f(f\ x)))\ (\lambda f.\lambda x.\ f\ x)
\]

\begin{verbatim}
12: eval ((λm.(λn.(m n))) (λf.(λx.(f (f x))))) (λf.(λx.(f x)))
    39: eval (λm.(λn.(m n))) (λf.(λx.(f (f x))))
        55: substitute ...
        60: eval (λn.(Var1 n))
            55: substitute ...
            39: eval (Var1 (λf.λx.f x))
                55: substitute ...
        ...
\end{verbatim}

Indentation matches the depth of the call stack, and each evaluation step follows the interpreter’s leftmost-outermost evaluation strategy.

\vspace{0.5em}

\textit{Item 8: Modifying the Interpreter}

The minimal non-terminating expression from Item 4 required a small modification so the interpreter could safely detect repeated unfolding. I added a simple check preventing infinite substitution loops by detecting repeated application patterns. The updated interpreter successfully executes all test cases, including the MWE.

\vspace{1em}

\textbf{Question:}  
The interpreter follows a leftmost-outermost evaluation strategy. If we switched to leftmost-innermost (applicative order), which test cases from this homework would change their behaviour, and why?

\section{Essay (Synthesis)}

Throughout this course, I came to see programming languages not merely as tools for writing code but as formal systems governed by structure, rules, and invariants. Each topic, from the MIU puzzle, to abstract reduction systems, to grammars, Lean, and finally the lambda calculus interpreter, revealed a different angle of the same underlying truth: computation is rule-guided symbolic transformation. This perspective not only reshaped my understanding of the mathematical foundations of programming, but also deepened my appreciation for the systems engineering work I performed during my internship this summer.

My internship involved working with safety-critical logic systems such as finite state machines, hardware interfaces, and automated test pipelines. I often had to reason about dependencies, invariants, and system behavior under strict constraints. Questions like ``Will this terminate'', ``Does this always reach the same state'', or ``Can these two processes conflict'' appeared constantly. At the time, these felt like practical engineering problems; through this course, I realized they are precisely the questions studied in confluence, termination, and invariant analysis within ARSs and the lambda calculus.

The MIU puzzle first introduced the idea that impossibility can be demonstrated using invariants. During my internship, traceability matrices, error-state diagrams, and state transition models served the same purpose; they ensured that certain unsafe or undesired system states could never be reached. ARSs then provided a general language for reasoning about these systems, explaining why some designs behave deterministically while others inherently diverge or branch.

We later studied grammars and parsing, which clarified something that consistently appears in systems engineering: structure determines interpretation. Just as precedence rules disambiguate arithmetic expressions, engineering specifications must be structurally precise so that machines interpret them correctly. Understanding grammars helped me recognize why a system’s documentation and interface definitions must be unambiguous.

Lean extended these themes into formal logic. Writing machine-checked proofs required articulating every logical dependency explicitly, something I had previously done informally when debugging or verifying system behavior during my internship. Lean made visible the chain of reasoning behind even simple claims, reinforcing the idea that correctness follows from many small, justified steps.

Finally, studying the lambda calculus brought everything together. Building the Python interpreter mirrored my engineering work: preventing variable capture felt like resolving naming collisions in test frameworks, tracing the evaluator resembled following recursive call stacks in embedded systems, and detecting non-terminating expressions paralleled guarding against infinite loops in automated processes. The need for precise substitution, careful evaluation rules, and consistent scope discipline gave me a concrete appreciation for how interpreters and engineered systems maintain internal logical coherence.

The main insight I gained is that formal reasoning is not separate from engineering practice; it is the underlying structure that makes engineering work at all. The tools from this course, such as invariants, confluence, termination, grammars, proofs, and substitution models, are the same tools used implicitly across real-world systems. This course provided the theoretical foundation beneath the intuition I developed during my internship, and together they have reshaped how I view computation, logic, and the design of reliable systems.


\section{Evidence of Participation}

Throughout the semester I actively participated in the course through a combination of online engagement, peer collaboration, and contributions to weekly discussions. My participation included:

\begin{itemize}
    \item Posting weekly ``interesting questions'' on the Discord channel that connected lecture material to broader themes in computation and systems design.
    \item Reviewing follow-up graded comments after graded assignments, reflecting on feedback and planning revisions for my final report.
    \item Engaging in discussions by responding to classmates’ questions and sharing clarifications or examples when relevant.
    \item Developing additional test cases for the lambda calculus interpreter, including examples involving variable capture, associativity, and non-terminating expressions.
    \item Exploring alternative proofs and reduction strategies in Lean beyond the minimal required steps, and sharing some of these explorations with classmates.
    \item Contributing to conversations about parsing, grammars, and evaluation strategies by relating them to practical systems engineering experiences with classmates.
\end{itemize}

These activities demonstrate consistent participation throughout the semester and engagement with both the technical and conceptual aspects of the course.

\section{Conclusion}\label{conclusion}
This course offered a unique opportunity to step back from day-to-day programming and examine computation through the lens of formal systems. Many computer science classes focus on building software, but this course focused on understanding the rules, structures, and mathematical constraints that make computation possible in the first place. By doing so, it provided a perspective that reaches far beyond programming languages and into the foundations of software engineering itself.

One of the most valuable aspects of the course was seeing how ideas reappeared across different domains. The MIU puzzle showed how invariants restrict what can be achieved in a rule-based system. ARSs generalized this into questions of confluence and termination, which reappeared in grammars through structural constraints, in Lean through proof normalization, and in the lambda calculus through beta-reduction. The unifying theme is that computation is not arbitrary; it follows precise rules that can be analyzed, predicted, and proven.

Connecting these concepts to the wider world of software engineering was particularly meaningful for me. During my systems engineering internship, I encountered many of the same ideas without realizing it. When designing state machines, verifying safety properties, or analyzing race conditions, I was implicitly working with invariants and reduction rules. When debugging recursive behavior or dealing with infinite loops in automated systems, I was essentially confronting non-termination. When enforcing strict formatting rules in interface documents or machine instructions, I was relying on context-free grammar structure. This course gave formal names, definitions, and theoretical explanations to the challenges I faced in practice.

The most interesting part of the course was building and modifying the lambda calculus interpreter. It served as a bridge between theory and implementation, showing that features we take for granted in programming languages, such as scope, substitution, and evaluation order, are built from surprisingly delicate mechanisms. Implementing capture-avoiding substitution and tracing evaluation with a debugger provided a new appreciation for how interpreters, compilers, and automated tools work internally.

In terms of improvements, the course could benefit from additional examples connecting formal concepts to real software systems. While the theoretical framing is essential, students might gain even more intuition by analyzing real-world scenarios, such as compiler optimizations, configuration languages, or protocol specifications, through the lens of ARSs or grammars. Even small case studies would help reinforce the relevance of the material.

Overall, the course expanded my understanding of computation, connected deeply with my engineering experience, and strengthened my appreciation for the formal rigor that underlies software systems. It provided both the conceptual clarity and the theoretical foundation that will continue to guide how I think about programming, logic, and reliable system design.


\begin{thebibliography}{99}
\bibitem{barendregt}
H.~Barendregt,
\textit{The Lambda Calculus: Its Syntax and Semantics}. 
North-Holland, 1984.

\bibitem{pierce}
B.~Pierce,
\textit{Types and Programming Languages}. 
MIT Press, 2002.

\bibitem{barton}
C.~Barton,
\textit{How I Wish I’d Taught Maths}. 
John Catt Educational, 2018.

\bibitem{lark}
Lark Parser Documentation, 
\url{https://github.com/lark-parser/lark}. 
Accessed 2025.

\bibitem{lean}
Lean Community,
\textit{The Natural Number Game and Tutorial World}. 
\url{https://leanprover-community.github.io/}. 
Accessed 2025.

\bibitem{python-docs}
Python Software Foundation,
\textit{Python 3 Documentation}. 
\url{https://docs.python.org/3/}. 
Accessed 2025.

\bibitem{harel}
D.~Harel,
``Statecharts: A Visual Formalism for Complex Systems'', 
\textit{Science of Computer Programming}, 8(3):231–274, 1987.

\bibitem{reactive}
D.~Harel and A.~Pnueli,
``On the Development of Reactive Systems'', 
\textit{Logics and Models of Concurrent Systems}. 
Springer, 1985.

\end{thebibliography}


\end{document}
